{
  "benchmark_id": "llm_speed:text_generation",
  "name": "LLM inference speed for text generation",
  "description": "Running llama-bench from llama.cpp using various quantized model files to measure the speed of generating 16 to 4k tokens.",
  "framework": "llm_speed",
  "config_fields": {
    "model": "Name of the model file used.",
    "tokens": "Number of tokens processed in one run.",
    "framework_version": "Git commit hash of llama.cpp"
  },
  "measurement": "text_generation",
  "unit": "tokens/second (t/s)",
  "higher_is_better": 1,
  "status": "ACTIVE"
}