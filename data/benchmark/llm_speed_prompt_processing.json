{
  "benchmark_id": "llm_speed:prompt_processing",
  "name": "LLM inference speed for prompt processing",
  "description": "Running llama-bench from llama.cpp using various quantized model files to measure the speed of processing 16 to 16k tokens.",
  "framework": "llm_speed",
  "config_fields": {
    "model": "Name of the model file used.",
    "tokens": "Number of tokens processed in one run.",
    "framework_version": "Git commit hash of llama.cpp"
  },
  "measurement": "prompt_processing",
  "unit": "tokens/second (t/s)",
  "higher_is_better": 1,
  "status": "ACTIVE"
}